{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOPi9333PE59",
    "outputId": "13a370eb-2a44-4858-c8e3-d6e2bbfa2030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJsLwj0-POgI",
    "outputId": "85aca787-dd80-4d0b-dfa8-fa62e25885c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: larq in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
      "Requirement already satisfied: terminaltables>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from larq) (3.1.0)\n",
      "Requirement already satisfied: importlib-metadata<4.0,>=2.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from larq) (3.10.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from larq) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0,>=2.0; python_version < \"3.8\"->larq) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0,>=2.0; python_version < \"3.8\"->larq) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "pip install larq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T5_x7AQ83oZi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from array import *\n",
    "import copy\n",
    "import random\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import larq as lq\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage import io\n",
    "import os\n",
    "import shutil\n",
    "tf.__version__\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJj7zT-T7jix",
    "outputId": "a073ab86-1001-4ff9-958b-27ef2d92fa51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d (QuantConv2D)      (None, 64, 64, 64)   9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 64)   0           quant_conv2d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 64)   256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_1 (QuantConv2D)    (None, 32, 32, 128)  8320        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         quant_conv2d_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_2 (QuantConv2D)    (None, 32, 32, 32)   36896       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 96)   0           quant_conv2d_2[0][0]             \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 96)   384         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_3 (QuantConv2D)    (None, 32, 32, 128)  12416       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         quant_conv2d_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_4 (QuantConv2D)    (None, 32, 32, 32)   36896       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           quant_conv2d_4[0][0]             \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_5 (QuantConv2D)    (None, 32, 32, 128)  16512       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         quant_conv2d_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_6 (QuantConv2D)    (None, 32, 32, 32)   36896       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 160)  0           quant_conv2d_6[0][0]             \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_7 (QuantConv2D)    (None, 32, 32, 128)  20608       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 128)  512         quant_conv2d_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_8 (QuantConv2D)    (None, 32, 32, 32)   36896       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 192)  0           quant_conv2d_8[0][0]             \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 192)  768         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_9 (QuantConv2D)    (None, 32, 32, 128)  24704       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         quant_conv2d_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_10 (QuantConv2D)   (None, 32, 32, 32)   36896       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 224)  0           quant_conv2d_10[0][0]            \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 224)  896         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_11 (QuantConv2D)   (None, 32, 32, 128)  28800       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 128)  512         quant_conv2d_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_12 (QuantConv2D)   (None, 32, 32, 32)   36896       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 256)  0           quant_conv2d_12[0][0]            \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_13 (QuantConv2D)   (None, 32, 32, 128)  32896       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 128)  0           quant_conv2d_13[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_14 (QuantConv2D)   (None, 16, 16, 128)  16512       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_14[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_15 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16, 16, 160)  0           quant_conv2d_15[0][0]            \n",
      "                                                                 average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 160)  640         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_16 (QuantConv2D)   (None, 16, 16, 128)  20608       batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_16[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_17 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 192)  0           quant_conv2d_17[0][0]            \n",
      "                                                                 concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 192)  768         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_18 (QuantConv2D)   (None, 16, 16, 128)  24704       batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_18[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_19 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 224)  0           quant_conv2d_19[0][0]            \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 224)  896         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_20 (QuantConv2D)   (None, 16, 16, 128)  28800       batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_20[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_21 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16, 16, 256)  0           quant_conv2d_21[0][0]            \n",
      "                                                                 concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 256)  1024        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_22 (QuantConv2D)   (None, 16, 16, 128)  32896       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_22[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_23 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 16, 16, 288)  0           quant_conv2d_23[0][0]            \n",
      "                                                                 concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 288)  1152        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_24 (QuantConv2D)   (None, 16, 16, 128)  36992       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_24[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_25 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 320)  0           quant_conv2d_25[0][0]            \n",
      "                                                                 concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 320)  1280        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_26 (QuantConv2D)   (None, 16, 16, 128)  41088       batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_26[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_27 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 352)  0           quant_conv2d_27[0][0]            \n",
      "                                                                 concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 352)  1408        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_28 (QuantConv2D)   (None, 16, 16, 128)  45184       batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_28[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_29 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 384)  0           quant_conv2d_29[0][0]            \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 384)  1536        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_30 (QuantConv2D)   (None, 16, 16, 128)  49280       batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_30[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_31 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 416)  0           quant_conv2d_31[0][0]            \n",
      "                                                                 concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 416)  1664        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_32 (QuantConv2D)   (None, 16, 16, 128)  53376       batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_33 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 448)  0           quant_conv2d_33[0][0]            \n",
      "                                                                 concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 448)  1792        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_34 (QuantConv2D)   (None, 16, 16, 128)  57472       batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_34[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_35 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 480)  0           quant_conv2d_35[0][0]            \n",
      "                                                                 concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 480)  1920        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_36 (QuantConv2D)   (None, 16, 16, 128)  61568       batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 128)  512         quant_conv2d_36[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_37 (QuantConv2D)   (None, 16, 16, 32)   36896       batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 512)  0           quant_conv2d_37[0][0]            \n",
      "                                                                 concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 512)  2048        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_38 (QuantConv2D)   (None, 16, 16, 256)  131328      batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 256)    0           quant_conv2d_38[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 256)    1024        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_39 (QuantConv2D)   (None, 8, 8, 128)    32896       batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_39[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_40 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 8, 8, 288)    0           quant_conv2d_40[0][0]            \n",
      "                                                                 average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 288)    1152        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_41 (QuantConv2D)   (None, 8, 8, 128)    36992       batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_41[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_42 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 8, 8, 320)    0           quant_conv2d_42[0][0]            \n",
      "                                                                 concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 320)    1280        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_43 (QuantConv2D)   (None, 8, 8, 128)    41088       batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_43[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_44 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 352)    0           quant_conv2d_44[0][0]            \n",
      "                                                                 concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 352)    1408        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_45 (QuantConv2D)   (None, 8, 8, 128)    45184       batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_45[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_46 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 384)    0           quant_conv2d_46[0][0]            \n",
      "                                                                 concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 384)    1536        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_47 (QuantConv2D)   (None, 8, 8, 128)    49280       batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_47[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_48 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 8, 8, 416)    0           quant_conv2d_48[0][0]            \n",
      "                                                                 concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 8, 416)    1664        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_49 (QuantConv2D)   (None, 8, 8, 128)    53376       batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_49[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_50 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 8, 8, 448)    0           quant_conv2d_50[0][0]            \n",
      "                                                                 concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 8, 8, 448)    1792        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_51 (QuantConv2D)   (None, 8, 8, 128)    57472       batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_51[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_52 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 480)    0           quant_conv2d_52[0][0]            \n",
      "                                                                 concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 480)    1920        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_53 (QuantConv2D)   (None, 8, 8, 128)    61568       batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_53[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_54 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 512)    0           quant_conv2d_54[0][0]            \n",
      "                                                                 concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 8, 8, 512)    2048        concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_55 (QuantConv2D)   (None, 8, 8, 128)    65664       batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_55[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_56 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 544)    0           quant_conv2d_56[0][0]            \n",
      "                                                                 concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 8, 8, 544)    2176        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_57 (QuantConv2D)   (None, 8, 8, 128)    69760       batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_57[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_58 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 576)    0           quant_conv2d_58[0][0]            \n",
      "                                                                 concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 8, 8, 576)    2304        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_59 (QuantConv2D)   (None, 8, 8, 128)    73856       batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_59[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_60 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 608)    0           quant_conv2d_60[0][0]            \n",
      "                                                                 concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 8, 8, 608)    2432        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_61 (QuantConv2D)   (None, 8, 8, 128)    77952       batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_61[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_62 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 640)    0           quant_conv2d_62[0][0]            \n",
      "                                                                 concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 8, 8, 640)    2560        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_63 (QuantConv2D)   (None, 8, 8, 128)    82048       batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_63[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_64 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 672)    0           quant_conv2d_64[0][0]            \n",
      "                                                                 concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 8, 8, 672)    2688        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_65 (QuantConv2D)   (None, 8, 8, 128)    86144       batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_65[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_66 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 8, 8, 704)    0           quant_conv2d_66[0][0]            \n",
      "                                                                 concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 8, 704)    2816        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_67 (QuantConv2D)   (None, 8, 8, 128)    90240       batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_67[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_68 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 736)    0           quant_conv2d_68[0][0]            \n",
      "                                                                 concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 8, 736)    2944        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_69 (QuantConv2D)   (None, 8, 8, 128)    94336       batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_69[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_70 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 768)    0           quant_conv2d_70[0][0]            \n",
      "                                                                 concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 8, 768)    3072        concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_71 (QuantConv2D)   (None, 8, 8, 128)    98432       batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_71[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_72 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 800)    0           quant_conv2d_72[0][0]            \n",
      "                                                                 concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 800)    3200        concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_73 (QuantConv2D)   (None, 8, 8, 128)    102528      batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_73[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_74 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 832)    0           quant_conv2d_74[0][0]            \n",
      "                                                                 concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 8, 832)    3328        concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_75 (QuantConv2D)   (None, 8, 8, 128)    106624      batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_75[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_76 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 8, 8, 864)    0           quant_conv2d_76[0][0]            \n",
      "                                                                 concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 864)    3456        concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_77 (QuantConv2D)   (None, 8, 8, 128)    110720      batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_77[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_78 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 8, 8, 896)    0           quant_conv2d_78[0][0]            \n",
      "                                                                 concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 896)    3584        concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_79 (QuantConv2D)   (None, 8, 8, 128)    114816      batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_79[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_80 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 8, 8, 928)    0           quant_conv2d_80[0][0]            \n",
      "                                                                 concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 928)    3712        concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_81 (QuantConv2D)   (None, 8, 8, 128)    118912      batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_81[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_82 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 8, 8, 960)    0           quant_conv2d_82[0][0]            \n",
      "                                                                 concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 960)    3840        concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_83 (QuantConv2D)   (None, 8, 8, 128)    123008      batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_83[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_84 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 8, 8, 992)    0           quant_conv2d_84[0][0]            \n",
      "                                                                 concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 992)    3968        concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_85 (QuantConv2D)   (None, 8, 8, 128)    127104      batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_85[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_86 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 8, 8, 1024)   0           quant_conv2d_86[0][0]            \n",
      "                                                                 concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 1024)   4096        concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_87 (QuantConv2D)   (None, 8, 8, 128)    131200      batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_87[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_88 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 8, 8, 1056)   0           quant_conv2d_88[0][0]            \n",
      "                                                                 concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 1056)   4224        concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_89 (QuantConv2D)   (None, 8, 8, 128)    135296      batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_89[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_90 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 8, 8, 1088)   0           quant_conv2d_90[0][0]            \n",
      "                                                                 concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 1088)   4352        concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_91 (QuantConv2D)   (None, 8, 8, 128)    139392      batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_91[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_92 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 8, 8, 1120)   0           quant_conv2d_92[0][0]            \n",
      "                                                                 concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 1120)   4480        concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_93 (QuantConv2D)   (None, 8, 8, 128)    143488      batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_93[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_94 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 8, 8, 1152)   0           quant_conv2d_94[0][0]            \n",
      "                                                                 concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 1152)   4608        concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_95 (QuantConv2D)   (None, 8, 8, 128)    147584      batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_95[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_96 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 8, 8, 1184)   0           quant_conv2d_96[0][0]            \n",
      "                                                                 concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 8, 8, 1184)   4736        concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_97 (QuantConv2D)   (None, 8, 8, 128)    151680      batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_97[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_98 (QuantConv2D)   (None, 8, 8, 32)     36896       batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 8, 8, 1216)   0           quant_conv2d_98[0][0]            \n",
      "                                                                 concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 8, 8, 1216)   4864        concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_99 (QuantConv2D)   (None, 8, 8, 128)    155776      batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 8, 8, 128)    512         quant_conv2d_99[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_100 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 8, 8, 1248)   0           quant_conv2d_100[0][0]           \n",
      "                                                                 concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 8, 8, 1248)   4992        concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_101 (QuantConv2D)  (None, 8, 8, 128)    159872      batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 8, 8, 128)    512         quant_conv2d_101[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_102 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 8, 8, 1280)   0           quant_conv2d_102[0][0]           \n",
      "                                                                 concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 8, 8, 1280)   5120        concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_103 (QuantConv2D)  (None, 8, 8, 128)    163968      batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 8, 8, 128)    512         quant_conv2d_103[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_104 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 8, 8, 1312)   0           quant_conv2d_104[0][0]           \n",
      "                                                                 concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 8, 8, 1312)   5248        concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_105 (QuantConv2D)  (None, 8, 8, 128)    168064      batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 8, 8, 128)    512         quant_conv2d_105[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_106 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 8, 8, 1344)   0           quant_conv2d_106[0][0]           \n",
      "                                                                 concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 8, 8, 1344)   5376        concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_107 (QuantConv2D)  (None, 8, 8, 128)    172160      batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 8, 8, 128)    512         quant_conv2d_107[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_108 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 8, 8, 1376)   0           quant_conv2d_108[0][0]           \n",
      "                                                                 concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 8, 8, 1376)   5504        concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_109 (QuantConv2D)  (None, 8, 8, 128)    176256      batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 8, 8, 128)    512         quant_conv2d_109[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_110 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 8, 8, 1408)   0           quant_conv2d_110[0][0]           \n",
      "                                                                 concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 8, 8, 1408)   5632        concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_111 (QuantConv2D)  (None, 8, 8, 128)    180352      batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 8, 8, 128)    512         quant_conv2d_111[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_112 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 8, 8, 1440)   0           quant_conv2d_112[0][0]           \n",
      "                                                                 concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 8, 8, 1440)   5760        concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_113 (QuantConv2D)  (None, 8, 8, 128)    184448      batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 8, 8, 128)    512         quant_conv2d_113[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_114 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 8, 8, 1472)   0           quant_conv2d_114[0][0]           \n",
      "                                                                 concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 8, 8, 1472)   5888        concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_115 (QuantConv2D)  (None, 8, 8, 128)    188544      batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 8, 8, 128)    512         quant_conv2d_115[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_116 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 8, 8, 1504)   0           quant_conv2d_116[0][0]           \n",
      "                                                                 concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 8, 8, 1504)   6016        concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_117 (QuantConv2D)  (None, 8, 8, 128)    192640      batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 8, 8, 128)    512         quant_conv2d_117[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_118 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 8, 8, 1536)   0           quant_conv2d_118[0][0]           \n",
      "                                                                 concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 8, 8, 1536)   6144        concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_119 (QuantConv2D)  (None, 8, 8, 128)    196736      batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 8, 8, 128)    512         quant_conv2d_119[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_120 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 8, 8, 1568)   0           quant_conv2d_120[0][0]           \n",
      "                                                                 concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 8, 8, 1568)   6272        concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_121 (QuantConv2D)  (None, 8, 8, 128)    200832      batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 8, 8, 128)    512         quant_conv2d_121[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_122 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 8, 8, 1600)   0           quant_conv2d_122[0][0]           \n",
      "                                                                 concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 8, 8, 1600)   6400        concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_123 (QuantConv2D)  (None, 8, 8, 128)    204928      batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 8, 8, 128)    512         quant_conv2d_123[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_124 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 8, 8, 1632)   0           quant_conv2d_124[0][0]           \n",
      "                                                                 concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 8, 8, 1632)   6528        concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_125 (QuantConv2D)  (None, 8, 8, 128)    209024      batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 8, 8, 128)    512         quant_conv2d_125[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_126 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 8, 8, 1664)   0           quant_conv2d_126[0][0]           \n",
      "                                                                 concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 8, 8, 1664)   6656        concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_127 (QuantConv2D)  (None, 8, 8, 128)    213120      batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 8, 8, 128)    512         quant_conv2d_127[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_128 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 8, 8, 1696)   0           quant_conv2d_128[0][0]           \n",
      "                                                                 concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 8, 8, 1696)   6784        concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_129 (QuantConv2D)  (None, 8, 8, 128)    217216      batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 8, 8, 128)    512         quant_conv2d_129[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_130 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 8, 8, 1728)   0           quant_conv2d_130[0][0]           \n",
      "                                                                 concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 8, 8, 1728)   6912        concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_131 (QuantConv2D)  (None, 8, 8, 128)    221312      batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 8, 8, 128)    512         quant_conv2d_131[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_132 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 8, 8, 1760)   0           quant_conv2d_132[0][0]           \n",
      "                                                                 concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 8, 8, 1760)   7040        concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_133 (QuantConv2D)  (None, 8, 8, 128)    225408      batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 8, 8, 128)    512         quant_conv2d_133[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_134 (QuantConv2D)  (None, 8, 8, 32)     36896       batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 8, 8, 1792)   0           quant_conv2d_134[0][0]           \n",
      "                                                                 concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 8, 8, 1792)   7168        concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_135 (QuantConv2D)  (None, 8, 8, 896)    1606528     batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 896)    0           quant_conv2d_135[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 4, 4, 896)    3584        average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_136 (QuantConv2D)  (None, 4, 4, 128)    114816      batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 4, 4, 128)    512         quant_conv2d_136[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_137 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 4, 4, 928)    0           quant_conv2d_137[0][0]           \n",
      "                                                                 average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 4, 4, 928)    3712        concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_138 (QuantConv2D)  (None, 4, 4, 128)    118912      batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 4, 4, 128)    512         quant_conv2d_138[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_139 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 4, 4, 960)    0           quant_conv2d_139[0][0]           \n",
      "                                                                 concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 4, 4, 960)    3840        concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_140 (QuantConv2D)  (None, 4, 4, 128)    123008      batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 4, 4, 128)    512         quant_conv2d_140[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_141 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 4, 4, 992)    0           quant_conv2d_141[0][0]           \n",
      "                                                                 concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 4, 4, 992)    3968        concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_142 (QuantConv2D)  (None, 4, 4, 128)    127104      batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 4, 4, 128)    512         quant_conv2d_142[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_143 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 4, 4, 1024)   0           quant_conv2d_143[0][0]           \n",
      "                                                                 concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 4, 4, 1024)   4096        concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_144 (QuantConv2D)  (None, 4, 4, 128)    131200      batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 4, 4, 128)    512         quant_conv2d_144[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_145 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 4, 4, 1056)   0           quant_conv2d_145[0][0]           \n",
      "                                                                 concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 4, 4, 1056)   4224        concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_146 (QuantConv2D)  (None, 4, 4, 128)    135296      batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 4, 4, 128)    512         quant_conv2d_146[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_147 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 4, 4, 1088)   0           quant_conv2d_147[0][0]           \n",
      "                                                                 concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 4, 4, 1088)   4352        concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_148 (QuantConv2D)  (None, 4, 4, 128)    139392      batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 4, 4, 128)    512         quant_conv2d_148[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_149 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 4, 4, 1120)   0           quant_conv2d_149[0][0]           \n",
      "                                                                 concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 4, 4, 1120)   4480        concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_150 (QuantConv2D)  (None, 4, 4, 128)    143488      batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 4, 4, 128)    512         quant_conv2d_150[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_151 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 4, 4, 1152)   0           quant_conv2d_151[0][0]           \n",
      "                                                                 concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 4, 4, 1152)   4608        concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_152 (QuantConv2D)  (None, 4, 4, 128)    147584      batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 4, 4, 128)    512         quant_conv2d_152[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_153 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 4, 4, 1184)   0           quant_conv2d_153[0][0]           \n",
      "                                                                 concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 4, 4, 1184)   4736        concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_154 (QuantConv2D)  (None, 4, 4, 128)    151680      batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 4, 4, 128)    512         quant_conv2d_154[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_155 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 4, 4, 1216)   0           quant_conv2d_155[0][0]           \n",
      "                                                                 concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 4, 4, 1216)   4864        concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_156 (QuantConv2D)  (None, 4, 4, 128)    155776      batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 4, 4, 128)    512         quant_conv2d_156[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_157 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 4, 4, 1248)   0           quant_conv2d_157[0][0]           \n",
      "                                                                 concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 4, 4, 1248)   4992        concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_158 (QuantConv2D)  (None, 4, 4, 128)    159872      batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 4, 4, 128)    512         quant_conv2d_158[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_159 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 4, 4, 1280)   0           quant_conv2d_159[0][0]           \n",
      "                                                                 concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 4, 4, 1280)   5120        concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_160 (QuantConv2D)  (None, 4, 4, 128)    163968      batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 4, 4, 128)    512         quant_conv2d_160[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_161 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 4, 4, 1312)   0           quant_conv2d_161[0][0]           \n",
      "                                                                 concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 4, 4, 1312)   5248        concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_162 (QuantConv2D)  (None, 4, 4, 128)    168064      batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 4, 4, 128)    512         quant_conv2d_162[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_163 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 4, 4, 1344)   0           quant_conv2d_163[0][0]           \n",
      "                                                                 concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 4, 4, 1344)   5376        concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_164 (QuantConv2D)  (None, 4, 4, 128)    172160      batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 4, 4, 128)    512         quant_conv2d_164[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_165 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 4, 4, 1376)   0           quant_conv2d_165[0][0]           \n",
      "                                                                 concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 4, 4, 1376)   5504        concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_166 (QuantConv2D)  (None, 4, 4, 128)    176256      batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 4, 4, 128)    512         quant_conv2d_166[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_167 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 4, 4, 1408)   0           quant_conv2d_167[0][0]           \n",
      "                                                                 concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 4, 4, 1408)   5632        concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_168 (QuantConv2D)  (None, 4, 4, 128)    180352      batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 4, 4, 128)    512         quant_conv2d_168[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_169 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 4, 4, 1440)   0           quant_conv2d_169[0][0]           \n",
      "                                                                 concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 4, 4, 1440)   5760        concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_170 (QuantConv2D)  (None, 4, 4, 128)    184448      batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 4, 4, 128)    512         quant_conv2d_170[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_171 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 4, 4, 1472)   0           quant_conv2d_171[0][0]           \n",
      "                                                                 concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 4, 4, 1472)   5888        concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_172 (QuantConv2D)  (None, 4, 4, 128)    188544      batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 4, 4, 128)    512         quant_conv2d_172[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_173 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 4, 4, 1504)   0           quant_conv2d_173[0][0]           \n",
      "                                                                 concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 4, 4, 1504)   6016        concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_174 (QuantConv2D)  (None, 4, 4, 128)    192640      batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 4, 4, 128)    512         quant_conv2d_174[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_175 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 4, 4, 1536)   0           quant_conv2d_175[0][0]           \n",
      "                                                                 concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 4, 4, 1536)   6144        concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_176 (QuantConv2D)  (None, 4, 4, 128)    196736      batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 4, 4, 128)    512         quant_conv2d_176[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_177 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 4, 4, 1568)   0           quant_conv2d_177[0][0]           \n",
      "                                                                 concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 4, 4, 1568)   6272        concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_178 (QuantConv2D)  (None, 4, 4, 128)    200832      batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 4, 4, 128)    512         quant_conv2d_178[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_179 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 4, 4, 1600)   0           quant_conv2d_179[0][0]           \n",
      "                                                                 concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 4, 4, 1600)   6400        concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_180 (QuantConv2D)  (None, 4, 4, 128)    204928      batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 4, 4, 128)    512         quant_conv2d_180[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_181 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 4, 4, 1632)   0           quant_conv2d_181[0][0]           \n",
      "                                                                 concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 4, 4, 1632)   6528        concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_182 (QuantConv2D)  (None, 4, 4, 128)    209024      batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 4, 4, 128)    512         quant_conv2d_182[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_183 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 4, 4, 1664)   0           quant_conv2d_183[0][0]           \n",
      "                                                                 concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 4, 4, 1664)   6656        concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_184 (QuantConv2D)  (None, 4, 4, 128)    213120      batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 4, 4, 128)    512         quant_conv2d_184[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_185 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_90 (Concatenate)    (None, 4, 4, 1696)   0           quant_conv2d_185[0][0]           \n",
      "                                                                 concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 4, 4, 1696)   6784        concatenate_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_186 (QuantConv2D)  (None, 4, 4, 128)    217216      batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 4, 4, 128)    512         quant_conv2d_186[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_187 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_91 (Concatenate)    (None, 4, 4, 1728)   0           quant_conv2d_187[0][0]           \n",
      "                                                                 concatenate_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 4, 4, 1728)   6912        concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_188 (QuantConv2D)  (None, 4, 4, 128)    221312      batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 4, 4, 128)    512         quant_conv2d_188[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_189 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 4, 4, 1760)   0           quant_conv2d_189[0][0]           \n",
      "                                                                 concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 4, 4, 1760)   7040        concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_190 (QuantConv2D)  (None, 4, 4, 128)    225408      batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 4, 4, 128)    512         quant_conv2d_190[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_191 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 4, 4, 1792)   0           quant_conv2d_191[0][0]           \n",
      "                                                                 concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 4, 4, 1792)   7168        concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_192 (QuantConv2D)  (None, 4, 4, 128)    229504      batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 4, 4, 128)    512         quant_conv2d_192[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_193 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 4, 4, 1824)   0           quant_conv2d_193[0][0]           \n",
      "                                                                 concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 4, 4, 1824)   7296        concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_194 (QuantConv2D)  (None, 4, 4, 128)    233600      batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 4, 4, 128)    512         quant_conv2d_194[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_195 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 4, 4, 1856)   0           quant_conv2d_195[0][0]           \n",
      "                                                                 concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 4, 4, 1856)   7424        concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_196 (QuantConv2D)  (None, 4, 4, 128)    237696      batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 4, 4, 128)    512         quant_conv2d_196[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_197 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 4, 4, 1888)   0           quant_conv2d_197[0][0]           \n",
      "                                                                 concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 4, 4, 1888)   7552        concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_198 (QuantConv2D)  (None, 4, 4, 128)    241792      batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 4, 4, 128)    512         quant_conv2d_198[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "quant_conv2d_199 (QuantConv2D)  (None, 4, 4, 32)     36896       batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 4, 4, 1920)   0           quant_conv2d_199[0][0]           \n",
      "                                                                 concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1920)         0           concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "quant_dense (QuantDense)        (None, 2)            3842        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 2)            0           quant_dense[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 18,334,914\n",
      "Trainable params: 18,109,826\n",
      "Non-trainable params: 225,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Dense\n",
    "from tensorflow.keras.layers import AvgPool2D, GlobalAveragePooling2D, MaxPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import ReLU, concatenate\n",
    "import tensorflow.keras.backend as K\n",
    "# Creating Densenet121\n",
    "def densenet(input_shape, n_classes, filters = 32):\n",
    "    \n",
    "    #batch norm + relu + conv\n",
    "    def bn_rl_conv(x,filters,kernel=1,strides=1):\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        #x = ReLU()(x)\n",
    "        x = lq.layers.QuantConv2D(filters, kernel, strides=strides, padding = 'same',\n",
    "                                  input_quantizer=\"ste_sign\", kernel_quantizer=\"ste_sign\",\n",
    "                                  kernel_constraint=\"weight_clip\")(x)\n",
    "        return x\n",
    "    \n",
    "    def dense_block(x, repetition):\n",
    "        \n",
    "        for _ in range(repetition):\n",
    "            y = bn_rl_conv(x, 4*filters)\n",
    "            y = bn_rl_conv(y, filters, 3)\n",
    "            x = concatenate([y,x])\n",
    "        return x\n",
    "        \n",
    "    def transition_layer(x):\n",
    "        \n",
    "        x = bn_rl_conv(x, K.int_shape(x)[-1] //2 )\n",
    "        x = AvgPool2D(2, strides = 2, padding = 'same')(x)\n",
    "        return x\n",
    "    \n",
    "    input = Input (input_shape)\n",
    "    x = lq.layers.QuantConv2D(64, 7, strides = 2, padding = 'same', kernel_quantizer=\"ste_sign\",\n",
    "               kernel_constraint=\"weight_clip\")(input)\n",
    "    x = MaxPool2D(3, strides = 2, padding = 'same')(x)\n",
    "    \n",
    "    for repetition in [6,12,48,32]:\n",
    "        \n",
    "        d = dense_block(x, repetition)\n",
    "        x = transition_layer(d)\n",
    "    x = GlobalAveragePooling2D()(d)\n",
    "    x = lq.layers.QuantDense(n_classes, input_quantizer=\"ste_sign\", kernel_quantizer=\"ste_sign\",\n",
    "                                  kernel_constraint=\"weight_clip\")(x)\n",
    "    output = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(input, output)\n",
    "    return model\n",
    "input_shape = 128, 128, 3\n",
    "n_classes = 2\n",
    "model = densenet(input_shape,n_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JMDe7BBkcVvZ"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MG4CBappQk-7"
   },
   "outputs": [],
   "source": [
    "y_train = np.load('/content/drive/MyDrive/pedestrian dataset/pedestrian_train_labels.npy')\n",
    "x_train = np.load('/content/drive/MyDrive/pedestrian dataset/pedestrian_train_images.npy', allow_pickle=True)\n",
    "y_test = np.load('/content/drive/MyDrive/pedestrian dataset/pedestrian_test_labels.npy')\n",
    "x_test = np.load('/content/drive/MyDrive/pedestrian dataset/pedestrian_test_images.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HLXbdBCsRWVQ"
   },
   "outputs": [],
   "source": [
    "x_train = (x_train/127.5) - 1.0\n",
    "x_test = (x_test/127.5) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MKrSB-uJRWYI"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoding = OneHotEncoder()\n",
    "y_train = encoding.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = encoding.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1jfcjsSSHbd",
    "outputId": "ce61105b-04c4-4ce3-8d88-63104b0995f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200, 128, 128, 3), (1200, 2), (741, 128, 128, 3), (741, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train), np.shape(y_train), np.shape(x_test), np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ThLUG5xXPdS3"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(zoom_range = 0.3, rotation_range=20, shear_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "train_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KYRh-C6X4cvR"
   },
   "outputs": [],
   "source": [
    "model_path = '/content/drive/MyDrive/pedestrian_densenet201_binarized_classifier2.h5'\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor = 'val_loss', save_best_only = True, mode = 'min', verbose = 1)\n",
    "\n",
    "#earlystop = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', verbose = 1, restore_best_weights = True,\n",
    "#                          min_delta = 0.001)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 4, verbose = 1, mode = 'min',\n",
    "                              min_delta = 0.0001)\n",
    "\n",
    "callback = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wX749_12g3rq",
    "outputId": "d4d6d62f-4286-4a35-8240-0de427c82882",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 61s 634ms/step - loss: 26.0166 - accuracy: 0.6525 - val_loss: 4.6030 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.60298, saving model to /content/drive/MyDrive/pedestrian_densenet201_binarized_classifier2.h5\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 13s 337ms/step - loss: 12.0553 - accuracy: 0.6575 - val_loss: 13.1510 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 4.60298\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 12s 327ms/step - loss: 7.2039 - accuracy: 0.6567 - val_loss: 1.2108 - val_accuracy: 0.5857\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.60298 to 1.21082, saving model to /content/drive/MyDrive/pedestrian_densenet201_binarized_classifier2.h5\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 13s 337ms/step - loss: 11.1202 - accuracy: 0.6475 - val_loss: 15.2749 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.21082\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 10.1063 - accuracy: 0.6408 - val_loss: 2.8042 - val_accuracy: 0.5911\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.21082\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 6.5041 - accuracy: 0.6517 - val_loss: 6.2301 - val_accuracy: 0.3941\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.21082\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 12s 315ms/step - loss: 10.0617 - accuracy: 0.6325 - val_loss: 19.1873 - val_accuracy: 0.4305\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.21082\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 10.8611 - accuracy: 0.5875 - val_loss: 7.5337 - val_accuracy: 0.5331\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.21082\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 4.8299 - accuracy: 0.6950 - val_loss: 10.1049 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.21082\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 5.0086 - accuracy: 0.6792 - val_loss: 6.8351 - val_accuracy: 0.3576\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.21082\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 4.3658 - accuracy: 0.6975 - val_loss: 4.1407 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.21082\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 3.0171 - accuracy: 0.7333 - val_loss: 0.9363 - val_accuracy: 0.6005\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.21082 to 0.93635, saving model to /content/drive/MyDrive/pedestrian_densenet201_binarized_classifier2.h5\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 12s 323ms/step - loss: 3.3229 - accuracy: 0.7042 - val_loss: 3.5955 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.93635\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 3.8363 - accuracy: 0.7167 - val_loss: 3.8363 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.93635\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 12s 315ms/step - loss: 2.8870 - accuracy: 0.7375 - val_loss: 3.0666 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.93635\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.3529 - accuracy: 0.7733 - val_loss: 1.5315 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.93635\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 13s 327ms/step - loss: 2.4834 - accuracy: 0.7558 - val_loss: 1.5316 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.93635\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.1813 - accuracy: 0.7433 - val_loss: 0.8748 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.93635 to 0.87482, saving model to /content/drive/MyDrive/pedestrian_densenet201_binarized_classifier2.h5\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 12s 323ms/step - loss: 2.1043 - accuracy: 0.7550 - val_loss: 11.1961 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.87482\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.5981 - accuracy: 0.7225 - val_loss: 3.6479 - val_accuracy: 0.6869\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.87482\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 13s 330ms/step - loss: 2.5367 - accuracy: 0.7458 - val_loss: 10.0426 - val_accuracy: 0.5897\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.87482\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 12s 315ms/step - loss: 2.5751 - accuracy: 0.7450 - val_loss: 2.2911 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.87482\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 12s 312ms/step - loss: 2.7250 - accuracy: 0.7467 - val_loss: 0.8747 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.87482 to 0.87470, saving model to /content/drive/MyDrive/pedestrian_densenet201_binarized_classifier2.h5\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 12s 320ms/step - loss: 2.6702 - accuracy: 0.7292 - val_loss: 0.7072 - val_accuracy: 0.3887\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.87470 to 0.70722, saving model to /content/drive/MyDrive/pedestrian_densenet201_binarized_classifier2.h5\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 12s 326ms/step - loss: 2.4105 - accuracy: 0.7433 - val_loss: 2.2911 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.70722\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 12s 317ms/step - loss: 2.3630 - accuracy: 0.7433 - val_loss: 2.2911 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.70722\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 13s 330ms/step - loss: 2.2062 - accuracy: 0.7450 - val_loss: 0.8747 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.70722\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 12s 316ms/step - loss: 2.4704 - accuracy: 0.7500 - val_loss: 0.8747 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 13s 331ms/step - loss: 2.5445 - accuracy: 0.7642 - val_loss: 0.8747 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.70722\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 12s 312ms/step - loss: 2.4579 - accuracy: 0.7567 - val_loss: 0.7072 - val_accuracy: 0.3887\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.70722\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.2851 - accuracy: 0.7533 - val_loss: 1.4284 - val_accuracy: 0.5735\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.70722\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.2708 - accuracy: 0.7483 - val_loss: 2.3356 - val_accuracy: 0.6491\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.3747 - accuracy: 0.7592 - val_loss: 2.8916 - val_accuracy: 0.6761\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.70722\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.1015 - accuracy: 0.7775 - val_loss: 2.3357 - val_accuracy: 0.7126\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.70722\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.3756 - accuracy: 0.7483 - val_loss: 2.7676 - val_accuracy: 0.7247\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.70722\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 13s 330ms/step - loss: 2.7301 - accuracy: 0.7325 - val_loss: 2.4457 - val_accuracy: 0.7422\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.3587 - accuracy: 0.7825 - val_loss: 2.3544 - val_accuracy: 0.7490\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.70722\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 1.9532 - accuracy: 0.7600 - val_loss: 2.0986 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.70722\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.3221 - accuracy: 0.7458 - val_loss: 1.9862 - val_accuracy: 0.5560\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.70722\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.0881 - accuracy: 0.7575 - val_loss: 2.3174 - val_accuracy: 0.7409\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.6116 - accuracy: 0.7275 - val_loss: 2.1912 - val_accuracy: 0.7598\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.70722\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.3649 - accuracy: 0.7567 - val_loss: 2.2984 - val_accuracy: 0.7409\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.70722\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.2299 - accuracy: 0.7583 - val_loss: 2.1597 - val_accuracy: 0.7503\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.70722\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.2893 - accuracy: 0.7533 - val_loss: 2.4009 - val_accuracy: 0.7274\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.4121 - accuracy: 0.7517 - val_loss: 2.0897 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.70722\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.7924 - accuracy: 0.7317 - val_loss: 2.5342 - val_accuracy: 0.7530\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.70722\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 12s 312ms/step - loss: 2.7662 - accuracy: 0.7617 - val_loss: 2.1499 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.70722\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.3040 - accuracy: 0.7667 - val_loss: 2.2339 - val_accuracy: 0.7409\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.2567 - accuracy: 0.7417 - val_loss: 2.3435 - val_accuracy: 0.7476\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.70722\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.2960 - accuracy: 0.7583 - val_loss: 2.4102 - val_accuracy: 0.7409\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.70722\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 12s 331ms/step - loss: 2.4785 - accuracy: 0.7408 - val_loss: 2.1792 - val_accuracy: 0.7665\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.70722\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.2164 - accuracy: 0.7700 - val_loss: 2.3231 - val_accuracy: 0.5560\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.3762 - accuracy: 0.7617 - val_loss: 2.2415 - val_accuracy: 0.7463\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.70722\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.0199 - accuracy: 0.7550 - val_loss: 2.5238 - val_accuracy: 0.5493\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.70722\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.1357 - accuracy: 0.7558 - val_loss: 2.3851 - val_accuracy: 0.7652\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.70722\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 13s 330ms/step - loss: 2.5017 - accuracy: 0.7550 - val_loss: 2.5960 - val_accuracy: 0.7476\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.7475 - accuracy: 0.7525 - val_loss: 2.3326 - val_accuracy: 0.5520\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.70722\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.4279 - accuracy: 0.7508 - val_loss: 2.3427 - val_accuracy: 0.5439\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.70722\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.1086 - accuracy: 0.7508 - val_loss: 2.3692 - val_accuracy: 0.5506\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.70722\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.4557 - accuracy: 0.7625 - val_loss: 2.5663 - val_accuracy: 0.7436\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 8.192000897078167e-13.\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 13s 328ms/step - loss: 2.1258 - accuracy: 0.7825 - val_loss: 2.1314 - val_accuracy: 0.7544\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.70722\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.2147 - accuracy: 0.7608 - val_loss: 2.4616 - val_accuracy: 0.7476\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.70722\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.4866 - accuracy: 0.7508 - val_loss: 2.4265 - val_accuracy: 0.7382\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.70722\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.3055 - accuracy: 0.7642 - val_loss: 2.4146 - val_accuracy: 0.7503\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.6384001360475466e-13.\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.3129 - accuracy: 0.7542 - val_loss: 2.2142 - val_accuracy: 0.7544\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.70722\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.3027 - accuracy: 0.7558 - val_loss: 2.4032 - val_accuracy: 0.7260\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.70722\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.1903 - accuracy: 0.7558 - val_loss: 2.5740 - val_accuracy: 0.7314\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.70722\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.3107 - accuracy: 0.7533 - val_loss: 2.2013 - val_accuracy: 0.5479\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 3.2768002178849846e-14.\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.2359 - accuracy: 0.7567 - val_loss: 2.4812 - val_accuracy: 0.7490\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.70722\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.1246 - accuracy: 0.7675 - val_loss: 2.5517 - val_accuracy: 0.7409\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.70722\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.4764 - accuracy: 0.7525 - val_loss: 2.3697 - val_accuracy: 0.7274\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.70722\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.5442 - accuracy: 0.7433 - val_loss: 2.0838 - val_accuracy: 0.7503\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 6.553600300244697e-15.\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.4839 - accuracy: 0.7458 - val_loss: 2.7091 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.70722\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 12s 316ms/step - loss: 2.5085 - accuracy: 0.7475 - val_loss: 2.5802 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.70722\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.4180 - accuracy: 0.7392 - val_loss: 2.2501 - val_accuracy: 0.7598\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.70722\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 1.9770 - accuracy: 0.7725 - val_loss: 2.2660 - val_accuracy: 0.7544\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.3107200431082805e-15.\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.4063 - accuracy: 0.7525 - val_loss: 2.4409 - val_accuracy: 0.7449\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.70722\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.3006 - accuracy: 0.7617 - val_loss: 2.1181 - val_accuracy: 0.7449\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.70722\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 12s 313ms/step - loss: 2.5191 - accuracy: 0.7708 - val_loss: 2.7466 - val_accuracy: 0.7328\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.70722\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.2826 - accuracy: 0.7383 - val_loss: 2.3026 - val_accuracy: 0.7463\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 2.6214401285682084e-16.\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.2719 - accuracy: 0.7483 - val_loss: 2.3245 - val_accuracy: 0.7584\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.70722\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.2424 - accuracy: 0.7783 - val_loss: 2.5463 - val_accuracy: 0.7530\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.70722\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.5039 - accuracy: 0.7600 - val_loss: 2.4212 - val_accuracy: 0.7355\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.70722\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 12s 314ms/step - loss: 2.2838 - accuracy: 0.7533 - val_loss: 2.1613 - val_accuracy: 0.7719\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 5.2428803630155353e-17.\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.5343 - accuracy: 0.7442 - val_loss: 2.4377 - val_accuracy: 0.7368\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.70722\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 2.1357 - accuracy: 0.7633 - val_loss: 2.6384 - val_accuracy: 0.7382\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.70722\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 13s 329ms/step - loss: 2.1131 - accuracy: 0.7542 - val_loss: 2.0296 - val_accuracy: 0.7719\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.70722\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 13s 331ms/step - loss: 2.3081 - accuracy: 0.7725 - val_loss: 2.4222 - val_accuracy: 0.7476\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.0485760990728867e-17.\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 12s 316ms/step - loss: 2.2098 - accuracy: 0.7792 - val_loss: 2.3003 - val_accuracy: 0.7530\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.70722\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 12s 316ms/step - loss: 2.3097 - accuracy: 0.7658 - val_loss: 2.6550 - val_accuracy: 0.7301\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.70722\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 12s 317ms/step - loss: 2.0967 - accuracy: 0.7725 - val_loss: 2.5853 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.70722\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 13s 333ms/step - loss: 2.4666 - accuracy: 0.7558 - val_loss: 2.6912 - val_accuracy: 0.7436\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 2.097152165058549e-18.\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 13s 332ms/step - loss: 2.7494 - accuracy: 0.7483 - val_loss: 2.2612 - val_accuracy: 0.7382\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.70722\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 13s 333ms/step - loss: 2.7056 - accuracy: 0.7500 - val_loss: 2.2654 - val_accuracy: 0.7422\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.70722\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 12s 319ms/step - loss: 2.3102 - accuracy: 0.7550 - val_loss: 2.4389 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.70722\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 13s 332ms/step - loss: 2.4463 - accuracy: 0.7450 - val_loss: 2.3151 - val_accuracy: 0.5574\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 4.19430449555322e-19.\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 13s 335ms/step - loss: 2.4520 - accuracy: 0.7358 - val_loss: 2.6146 - val_accuracy: 0.5439\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.70722\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 12s 317ms/step - loss: 2.1054 - accuracy: 0.7683 - val_loss: 2.4797 - val_accuracy: 0.5466\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.70722\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 13s 331ms/step - loss: 2.1639 - accuracy: 0.7475 - val_loss: 2.1354 - val_accuracy: 0.7611\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.70722\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 12s 317ms/step - loss: 2.4629 - accuracy: 0.7442 - val_loss: 2.3067 - val_accuracy: 0.7503\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.70722\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 8.388609197901593e-20.\n"
     ]
    }
   ],
   "source": [
    "epochs_history = model.fit(train_datagen.flow(x_train, y_train, batch_size=32), validation_data =\n",
    "                           (x_test, y_test), epochs = 100, callbacks = callback, verbose = 1, \n",
    "                           batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "wiaj8ZW9GJqY",
    "outputId": "70af69f6-45dd-45b2-b687-b2c6b7bfe9ea"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs_history.history['accuracy'], c = 'red', label=\"train_accuracy\")\n",
    "plt.plot(epochs_history.history['loss'], c = 'black', label = 'train_loss')\n",
    "plt.plot(epochs_history.history['val_accuracy'], c = 'brown', label = 'validation_accuracy')\n",
    "plt.plot(epochs_history.history['val_loss'], c = 'gray', label = 'validation_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss/accuracy')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title('loss and accuracy per epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bi5nq0RZDarG",
    "outputId": "692c6cbe-7006-45a8-ba84-a628214b7e7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 93ms/step - loss: 2.3067 - accuracy: 0.7503\n",
      "accuracy in test_set: 75.03 %\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(x_test, y_test)\n",
    "print('accuracy in test_set: %.2f' %(accuracy*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fXntJR4saFsV"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Fs9BfYk7aFuh",
    "outputId": "863e5d2d-2938-4eab-9338-eddf96453ad6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2ce823afd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUQ0lEQVR4nO3de7RXZZ3H8ff3d7iIICFaRIDpCF5wKFDHdFlTieZlHO02js4aIwfFSryVZppNNt3MLMuxnE6hYihIKclqtDKzHBUFREPASRkTAUkKFA0Wyjk888fZ4dHOFQ7nOXvzfrH24refvX97f39r4Wc9PvvZe0dKCUlS96vlLkCSdlQGsCRlYgBLUiYGsCRlYgBLUia9tvcJTojjnWahv3L2I9/LXYJ6oKPGDottPUZnMmd2+uk2n29b2AOWpEy2ew9YkrpTrUT9SgNYUqXURV3uEjrMAJZUKbXIOqzbKQawpEoJhyAkKQ97wJKUiT1gScrEHrAkZeIsCEnKxHnAkpSJQxCSlIkX4SQpk1oYwJKUhRfhJCmTGo4BS1IWZRoDLk+lktQBtYgOL22JiJ0iYm5E/DYiFkfEF4r2vSLioYhYGhG3RESfor1vsb602L5nu7V2we+VpB6j1ok/7XgZOCKl9HZgLHBMRBwKfA24KqU0EngemFjsPxF4vmi/qtivnVolqUIiosNLW1KTPxervYslAUcAPy7apwLvLz6fWKxTbB8f7ZzEAJZUKb2irsNLREyKiPnNlknNjxURdRHxKLAauAv4P+CFlFJDscsKYFjxeRiwHKDYvg7Yrc1au+5nS1J+nbkIl1KqB+rb2N4IjI2IQcAsYL9tLrAZA1hSpWyPW5FTSi9ExD3AYcCgiOhV9HKHAyuL3VYCI4AVEdELeAOwps1au7xSScooqHV4afM4EW8ser5ERD/gKOBx4B7gw8VuE4Dbi8+zi3WK7b9KKaW2zmEPWFKldOGtyEOBqRFRR1NndWZK6acRsQSYERFfAh4BphT7TwF+GBFLgbXAye2dwACWVClddStySmkhMK6F9qeAQ1po3wj8U2fOYQBLqhRvRZakTHwamiRlEvaAJSmTmgEsSXn4SiJJyiPqHAOWpDwcgpCkTAxgScqjvcdM9iQGsKRqsQcsSZnYA5akTOp8Lb0kZREOQUhSJgawJGXiGLAkZWIPWJLy8FZkScrFIQhJysQhCEnKxACWpDx8FoQk5WIPWJIycRaEJOXhEIQk5eIQxI6nd9/efPXer9G7b2/qetW4/8f3M/2ym/nktAsYefBIGjc18uTcJ/jOmdfQ2NC45XsjDx7F1+dcyddPvoIHbr0/4y/Q9jLt2itYtOBBdhk4iM9+4zoAZk37LxY9PIe6Xr3ZfchQ/vXjF7Fz/wE0NGxiev03eeapJ6hF8KGPTmafA8Zm/gUlU6IALs9gSQ+36eVNXHrEJZw79mzOHXsOBx5zEPu+Y19+c9Ov+cR+H+PsMWfRp18f3nf6+7Z8p1ar8dGvfZRHfvFIxsq1vR367qM56+LLX9O235iDuOTK67jk6z/gTUNH8Iuf3AzA/Xf/NwCfvXIKky/9OrN+eC2bN2/u9ppLLaLjS2btBnBE7BcRF0XE1cVyUUTs3x3Flc3G9RsBqOvdi16960gp8fCd87dsf2LuE+w2fPct68effTwP3PoA61a/0O21qvuMHP12dh4w8DVt+7/976grnlu716j9eWHNHwH4w4pl7Pu34wDY5Q270q//AJ556nfdW3DZ1aLjS+5S29oYERcBM4AA5hZLANMj4jPbv7xyqdVqfOuRq/nh6mk8etejPDH3iS3b6nrV8d5T38uCny0AYPBbduPQDxzGndfekatc9RBz7rmT0eMOAWDYW/fmsfkP0NjYyJ9Wr2L5U0/wfBHO6pioq3V4ya29MeCJwAEppU3NGyPim8Bi4PKWvhQRk4BJAG9jDG9ljy4otefbvHkz5407h/5v6M/Fsz7LHge8lWcWLwPgY9/9BIvvXcyS+xYDcMa3zmDqRTeQUspZsjL72W3TqNXV8XfvPBKAw957LM+tXMYVF3+MwW8cwl77HECtlj8oSqUHDC10VHsBvBl4C7Dsde1Di20tSinVA/UAJ8TxO1zCrF+3nsfuWciBxxzIM4uXcfK/n8Ib3jiQr555zZZ9Rh48kgtmfBqAgbsP5KDjDqaxoZGHbn8wV9nqZg/++mcsWvAg53zuyi1Tp+rq6vjQhLO27PONz03mTUOH5yqxnHrA0EJHtRfA5wF3R8STwPKibQ9gJDB5exZWNgN3H0jjpkbWr1tPn536MPaocdz6tR9z1MT3Me7oA/nc+M++prd7xt+cvuXzudefx7yfzjN8dyBLHp3LL2ffwrmXXUWfvjttaX/l5Y2klOi7Uz8eXzifWq2OocP3zFdoGXVR/kbECOBGYAiQgPqU0rcj4jLgDOAvY0OXpJTuKL5zMU0jB43AOSmln7d1jjYDOKX0s4jYBzgEGFY0rwTmpZQaW//mjmfw0MGcN/V8anU1olbjvpn/w/z/nsesTbezetlqrphzJQBzbnuAW744I3O16k7Xf/uLPLnkt/z5pXVc+vGTOO6fPsovfnIzDQ2buOZLFwKw56jRnHLG+by07gW+85VPE1Fj0ODdmTD54szVl1DXDUE0AJ9KKS2IiF2AhyPirmLbVSmlK1972hgNnAwcQNPIwS8jYp+2sjK29xjkjjgEofad/cj3cpegHuioscO2OT2vOHZqhzPn03dO6PD5IuJ24BrgcODPLQTwxQAppa8W6z8HLkspzWntmI7uS6qWTswDjohJETG/2TKp5UPGnsA44KGiaXJELIyI6yJi16JtGK8O1QKs4NWRgxYZwJKqpRPzgFNK9Smlg5st9a8/XEQMAG4FzkspvQhcC+wNjAVWAd/Y6lK39ouS1CNFJ5b2DhXRm6bwvSmldBtASum5lFJjSmkz8H2arpFB0/WxEc2+Prxoa5UBLKlauuhW5GiaGzgFeDyl9M1m7UOb7fYBYFHxeTZwckT0jYi9gFE03bzWKh/GI6lauq5beThwKvBYRDxatF0CnBIRY2mamvY0cCZASmlxRMwEltA0g+Ks9maLGcCSKiW66M7BlNJ9tDxQ0erzA1JKXwa+3NFzGMCSqqU8N8IZwJIqpkK3IktSuVToYTySVC7lyV8DWFLFOAQhSZkYwJKURxjAkpRJefLXAJZUMc6CkKRMHIKQpEwMYEnKpETPeDSAJVWLY8CSlEcYwJKUiUMQkpSJPWBJyqTOAJakPOwBS1ImBrAkZeJFOEnKxB6wJGXiRThJysQesCRlYgBLUiZehJOkTOwBS1Im5clfA1hSxdSVZwzCAJZULfaAJSkTX0kkSZl4EU6SMilP/pZpxpwkdUAtOr60ISJGRMQ9EbEkIhZHxLlF++CIuCsiniz+3rVoj4i4OiKWRsTCiDiw3VK75AdLUk/RRQEMNACfSimNBg4FzoqI0cBngLtTSqOAu4t1gGOBUcUyCbi23VK37hdKUg/VRQGcUlqVUlpQfH4JeBwYBpwITC12mwq8v/h8InBjavIgMCgihrZZ6tb/SknqgSI6vETEpIiY32yZ1PIhY09gHPAQMCSltKrY9AdgSPF5GLC82ddWFG2t8iKcpGrpRLcypVQP1Le1T0QMAG4Fzkspvdj8tfcppRQRaesKtQcsqWo60QNu/1DRm6bwvSmldFvR/NxfhhaKv1cX7SuBEc2+Prxoa5UBLKla6qLjSxuiqas7BXg8pfTNZptmAxOKzxOA25u1f6SYDXEosK7ZUEWLHIKQVC1ddyPG4cCpwGMR8WjRdglwOTAzIiYCy4CTim13AMcBS4ENwGntncAAllQtXRTAKaX7aP22jvEt7J+AszpzDgNYUrWUaGDVAJZULT4L4lUzG2Zv71OohK45fVbuEtQDHXX9h7b9IAawJOURvpZekjKxByxJeZQofw1gSdUSJUpgA1hStTgNTZLysAcsSZmEL+WUpEzsAUtSHvaAJSmX8uSvASypWrwIJ0mZOAQhSZnYA5akXLwRQ5LysAcsSbkYwJKUR4ny1wCWVC0+kF2SMnEMWJJyMYAlKY8S5a8BLKliSpTABrCkSvFWZEnKxACWpEycBSFJuZQnfw1gSdXiEIQkZVKe+C3Vg9skqX21WnR4aU9EXBcRqyNiUbO2yyJiZUQ8WizHNdt2cUQsjYjfRcTR7da61b9SknqgiI4vHXADcEwL7VellMYWyx1N543RwMnAAcV3vhsRdW0d3ACWVCnRiT/tSSndC6zt4KlPBGaklF5OKf0eWAoc0tYXDGBJldLFPeDWTI6IhcUQxa5F2zBgebN9VhRtrTKAJVVKZwI4IiZFxPxmy6QOnOJaYG9gLLAK+MbW1uosCEmV0pkbMVJK9UB9Z46fUnqu2bm+D/y0WF0JjGi26/CirVX2gCVVSi2iw8vWiIihzVY/APxlhsRs4OSI6BsRewGjgLltHcsesKRK6co7kSNiOvAeYPeIWAF8HnhPRIwFEvA0cCZASmlxRMwElgANwFkppca2jm8AS6qUrrwRI6V0SgvNU9rY/8vAlzt6fANYUqX4MB5JyqRE+WsAS6qWrb24loMBLKlSSpS/BrCkanEMWJIyKU/8GsCSKqZEHWADWFK1OAQhSZk4C0KSMilR/hrAkqrFAJakTGolmgdhAG9Hxx45np3796euVkddrzqm/+jHANw8bRq3TL+ZWq3G37/73Zx/wYWZK9X2ssvgfhx7+sH0H7gTCVj4m9+z4K6lHP6B0Ywc9xZSSmx48WXunDKf9S9sZP9DR3DIcftCwCsbG/jljY/wx+Xrcv+MUrEHrC1+cMNUdt111y3rcx96iF//6m5+NOsn9OnThzVr1mSsTtvb5sbEr295jNXLXqD3Tr049fNHsGzxc8y78wnun7UEgHFH7s1hJ+zPL298hHV/Ws+My3/Dyxs2sdeYIbxvwoHc9KV7Mv+KcjGA1aofzZjBv51+Bn369AFgt912y1yRtqf16zayft1GADZtbGDtqpcYMKgfa559acs+vfv2anqyLPDs0lff//js/61lwOB+3VpvFTgLQk0i+NjpE4kIPnzSP/Phk05i2dNPs+Dhh/nPb3+bvn378MkLP83fjhmTu1J1g4G77cyb9hjEqqeaQvadHzyA0YfvwSsbNnHLFff+1f5j/n5Pfv/YH7q7zNIr0zzgrX4lUUSc1sa2LS+6m/L9Tr1uqVJumHYTt9x6G9/5Xj23TL+Zh+fPo6GxgXXr1jFtxgzOv+BCLvzk+aSUcpeq7ax33zpOmHwo90z/La9sbADgvtsWU/+pO1ny4HLGjd/7NfuP2O+NjHnXntw7c1FLh1MbuumtyF1iW94J94XWNqSU6lNKB6eUDp54RkdeMlpNQ4YMAZqGGY4YfySLFj7GkDe/mfFHHUVEMOZtb6NWq/H8889nrlTbU60uOGHyYTw+ZzlPPvzsX21/fM4z7HPQq28v3334QI4+7UB+cvUcNq5/pTtLrYQyBXCbQxARsbC1TcCQri+nOjZs2EBKif79+7NhwwbmPHA/Z378E/TbeWfmzX2IQ97xDp5++vds2rTpNRfpVD1Hn3YQa599kYd/8eSWtkFDBvDCc38GYOS4t7B2VdOY8C6D+3Hi5MO44/vzeL7Yrs6JCk1DGwIcDby+ixbAA9uloopYu2YN559zNgANDQ0c9w/Hc/i73sWmV17h3y+9lA+e8I/07t2bL37lq6Uas1LnDBu1Gwcc/lb+uHwdH/nCeAD+59bFjHnXngx+8wBSghfXbOCuqQsAOOzE/ek3oA9HnjoOaJpFMe0/fpWt/jKq1crz31O0Nf4YEVOA61NK97Ww7eaU0r+0d4KNjZsd4NRfueb0WblLUA90wfUf2ub0vP9/n+tw5hy+35Csad1mDzilNLGNbe2GryR1tzL9H6XT0CRVSnni1wCWVDEl6gAbwJKqxSEIScrEW5ElKZMS5a8BLKlaHIKQpExKlL8GsKRqMYAlKZMqPQtCkkqlTM+C2JbHUUpSj9OVj6OMiOsiYnVELGrWNjgi7oqIJ4u/dy3aIyKujoilEbEwIg5s7/gGsKRKiU786YAbgGNe1/YZ4O6U0ijg7mId4FhgVLFMAq5t7+AGsKRK6coecErpXmDt65pPBKYWn6cC72/WfmNq8iAwKCKGtnV8A1hSpdQiOrw0f31asXTkFT5DUkqris9/4NWXUwwDljfbb0XR1iovwkmqlFonupUppXpgq19cmVJKEbHVzzy3ByypUrp4DLglz/1laKH4e3XRvhIY0Wy/4UVbqwxgSZXSDS/lnA1MKD5PAG5v1v6RYjbEocC6ZkMVLXIIQlKldOWzICJiOvAeYPeIWAF8HrgcmBkRE4FlwEnF7ncAxwFLgQ3Aae0d3wCWVCldeStySumUVjaNb2HfBJzVmeMbwJIqxaehSVImPpBdkjIpUf4awJKqpUT5awBLqpgSdYENYEmVUp74NYAlVUyJOsAGsKRqcRqaJGVSnvg1gCVVTIk6wAawpKopTwIbwJIqxR6wJGVSopciG8CSqqY8CWwAS6oUhyAkKZMS5a8BLKliSpTABrCkStmGl212OwNYUqU4C0KScinRVTgDWFKllCd+DWBJFVOiDrABLKlaSpS/BrCkiilRF9gAllQpzoKQpGzKk8AGsKRKKdEIhAEsqVpKlL8GsKRqsQcsSZmU6a3IkVLKXcMOIyImpZTqc9ehnsV/FzuuWu4CdjCTchegHsl/FzsoA1iSMjGAJSkTA7h7Oc6nlvjvYgflRThJysQesCRlYgBLUiYGcDeJiGMi4ncRsTQiPpO7HuUXEddFxOqIWJS7FuVhAHeDiKgDvgMcC4wGTomI0XmrUg9wA3BM7iKUjwHcPQ4BlqaUnkopvQLMAE7MXJMySyndC6zNXYfyMYC7xzBgebP1FUWbpB2YASxJmRjA3WMlMKLZ+vCiTdIOzADuHvOAURGxV0T0AU4GZmeuSVJmBnA3SCk1AJOBnwOPAzNTSovzVqXcImI6MAfYNyJWRMTE3DWpe3krsiRlYg9YkjIxgCUpEwNYkjIxgCUpEwNYkjIxgCUpEwNYkjL5f+XC2lZMIblVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis = 1), y_pred)\n",
    "sns.heatmap(cm, annot=True, cmap=\"BuPu\", fmt = 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQ4L7TceaFyQ",
    "outputId": "81ad13d8-3691-4292-9f9a-9dd9260ddacd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.72      0.78       453\n",
      "           1       0.64      0.81      0.71       288\n",
      "\n",
      "    accuracy                           0.75       741\n",
      "   macro avg       0.75      0.76      0.75       741\n",
      "weighted avg       0.77      0.75      0.75       741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "report = classification_report(np.argmax(y_test, axis = 1), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwSYdG4Lk80C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pedestrian Classification (binarized).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
